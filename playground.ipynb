{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "loop = asyncio.get_event_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from motor.motor_asyncio import AsyncIOMotorClient\n",
    "\n",
    "\n",
    "def create_client(host: str, username: str,\n",
    "                  password: str, port: int,\n",
    "                  db_name: str) -> AsyncIOMotorClient:\n",
    "    return AsyncIOMotorClient(\n",
    "            f\"mongodb://{username}:{password}@{host}:{port}/{db_name}?authSource=admin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "\n",
    "class AsyncCRUDBase(object):\n",
    "\n",
    "    @staticmethod\n",
    "    async def get(db: Any, query: Any, **kwargs):\n",
    "        return NotImplemented\n",
    "\n",
    "    @staticmethod\n",
    "    async def delete(db: Any, query: Any, **kwargs):\n",
    "        return NotImplemented\n",
    "\n",
    "    @staticmethod\n",
    "    async def insert_many(db: Any, data: Any, **kwargs):\n",
    "        return NotImplemented\n",
    "\n",
    "    async def save(self, db, collection):\n",
    "        return NotImplemented\n",
    "\n",
    "\n",
    "class AsyncMongoCRUDBase(AsyncCRUDBase):\n",
    "    \"\"\" Provides minimal support for writing to MongoDB\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    async def get(collection: Any,  query: Any, **kwargs) -> List[object]:\n",
    "        result = [data async for data in collection.find(query)]\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    async def delete(collection: Any, query: Any, **kwargs):\n",
    "        return NotImplemented\n",
    "\n",
    "    @staticmethod\n",
    "    async def insert_many(collection: Any, data: Any, **kwargs):\n",
    "        await collection.insert_many(data)\n",
    "\n",
    "    async def save(self, collection):\n",
    "        return NotImplemented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from bson import ObjectId\n",
    "from datetime import datetime, date\n",
    "from typing import Optional, List\n",
    "\n",
    "class MongoModel(BaseModel, AsyncMongoCRUDBase):\n",
    "\n",
    "    class Config:\n",
    "        allow_population_by_field_name = True\n",
    "        json_encoders = {\n",
    "            datetime: lambda dt: dt.isoformat(),\n",
    "            ObjectId: lambda oid: str(oid),\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_mongo(cls, data: dict):\n",
    "        \"\"\"We must convert _id into \"id\". \"\"\"\n",
    "        if not data:\n",
    "            return data\n",
    "        id = data.pop('_id', None)\n",
    "        return cls(**dict(data, id=id))\n",
    "\n",
    "    def mongo(self, **kwargs):\n",
    "        exclude_unset = kwargs.pop('exclude_unset', True)\n",
    "        by_alias = kwargs.pop('by_alias', True)\n",
    "\n",
    "        parsed = self.dict(\n",
    "            exclude_unset=exclude_unset,\n",
    "            by_alias=by_alias,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Mongo uses `_id` as default key. We should stick to that as well.\n",
    "        if '_id' not in parsed and 'id' in parsed:\n",
    "            parsed['_id'] = parsed.pop('id')\n",
    "\n",
    "        return parsed\n",
    "    \n",
    "    @staticmethod\n",
    "    async def insert_many(collection: Any, data: List[AsyncMongoCRUDBase], **kwargs):\n",
    "        await collection.insert_many([d.mongo() for d in data])\n",
    "\n",
    "    @classmethod\n",
    "    async def get(cls, collection: Any,  query: Any, **kwargs) -> List[object]:\n",
    "        result = [cls.from_mongo(data) async for data in collection.find(query)]\n",
    "        return result\n",
    "\n",
    "    async def save(self, db, collection_name:str):\n",
    "        try:\n",
    "            await db[collection_name].insert_one(self.mongo())\n",
    "        except Exception as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "domain_pattern = re.compile(\"^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/?\\n]+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class JobState(str, Enum):\n",
    "    PENDING= 'pending'\n",
    "    DONE = 'done'\n",
    "    WORKING = 'working'\n",
    "    FAILED = 'failed'\n",
    "\n",
    "\n",
    "class ContentType(str, Enum):\n",
    "    WEBPAGE: str = 'webpage'\n",
    "    IMAGE: str = 'image'\n",
    "    AUDIO: str = 'audio'\n",
    "    VIDEO: str = 'video'\n",
    "\n",
    "\n",
    "class JobType(str, Enum):\n",
    "    \"\"\" Job types supported by spiders\n",
    "\n",
    "    BASIC_PAGE_SCRAPING: only scrape the provided urls and return the html of those urls,\n",
    "    SEARCH_RESULT_AGGREGATION: perform searches on search engines or general search page and retrieve their results,\n",
    "    WEB_CRAWLING: Start from seed urls, follow all links available.\n",
    "    \"\"\"\n",
    "    BASIC_PAGE_SCRAPING: str = 'basic_page_scraping'\n",
    "    SEARCH_RESULT_AGGREGATION: str = 'search_result_aggregation'\n",
    "    # WEB_CRAWLING: str = 'web_crawling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordRules(BaseModel):\n",
    "    include: List[str] = []\n",
    "    exclude: List[str] = []\n",
    "\n",
    "\n",
    "class SizeLimit(BaseModel):\n",
    "    max_pages: Optional[int]\n",
    "    max_size: Optional[int]\n",
    "\n",
    "\n",
    "class TimeRange(BaseModel):\n",
    "    past_days: Optional[int]\n",
    "    date_before: Optional[date]\n",
    "    date_after: Optional[date]\n",
    "\n",
    "\n",
    "class RegexPattern(BaseModel):\n",
    "    patterns: Optional[List[str]] = []\n",
    "\n",
    "\n",
    "class ScrapeRules(BaseModel):\n",
    "    \"\"\" Describes rules a spider should follow\n",
    "\n",
    "    Fields:\n",
    "        keywords: Optional[KeywordRules]\n",
    "        size_limit: Optional[SizeLimit]\n",
    "        time_range: Optional[TimeRange]\n",
    "        regular_expressions: Optional[RegexPattern]\n",
    "        max_retry: Optional[int] = 1  \n",
    "    \"\"\"\n",
    "    keywords: Optional[KeywordRules]\n",
    "    size_limit: Optional[SizeLimit]\n",
    "    time_range: Optional[TimeRange]\n",
    "    regular_expressions: Optional[RegexPattern]\n",
    "    max_retry: Optional[int] = 1\n",
    "\n",
    "\n",
    "\n",
    "class JobSpecification(BaseModel):\n",
    "    \"\"\" Describes what kind of task a spider should perform\n",
    "\n",
    "    Fields:\n",
    "        urls: List[str]\n",
    "        job_type: JobType\n",
    "        scrape_rules: ScrapeRules\n",
    "        data_collection: str = 'test'\n",
    "    \"\"\"\n",
    "    urls: List[str]\n",
    "    job_type: JobType\n",
    "    scrape_rules: ScrapeRules\n",
    "    data_collection: str = 'test'\n",
    "    job_collection: str = \"jobs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Any\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class JobStatus(MongoModel):\n",
    "    job_id: str\n",
    "    create_dt: datetime\n",
    "    page_count: int = 0\n",
    "    time_consumed: Optional[timedelta]\n",
    "    current_state: JobState\n",
    "    specification: JobSpecification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_spec = JobSpecification(\n",
    "    urls=['http://www.qq.com',\n",
    "          \"http://www.taobao.com\",\n",
    "          \"http://www.baidu.com\",\n",
    "          'http://www.guancha.cn',\n",
    "          'http://www.sina.com.cn']*5,\n",
    "    job_type=JobType.BASIC_PAGE_SCRAPING,\n",
    "    scrape_rules=ScrapeRules(\n",
    "        sizelimit=SizeLimit(max_pages=10)\n",
    "    )\n",
    ")\n",
    "job_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "job_status = JobStatus(\n",
    "            job_id=str(uuid4()),\n",
    "            create_dt=datetime.now(),\n",
    "            page_count=0,\n",
    "            specification=job_spec,\n",
    "            current_state=JobState.PENDING,\n",
    "            time_consumed=timedelta(seconds=0))\n",
    "job_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "\n",
    "def timeit(func):\n",
    "    async def process(func, *args, **params):\n",
    "        if asyncio.iscoroutinefunction(func):\n",
    "            print('this function is a coroutine: {}'.format(func.__name__))\n",
    "            return await func(*args, **params)\n",
    "        else:\n",
    "            print('this is not a coroutine')\n",
    "            return func(*args, **params)\n",
    "\n",
    "    async def helper(*args, **params):\n",
    "        print('{}.time'.format(func.__name__))\n",
    "        start = time.time()\n",
    "        result = await process(func, *args, **params)\n",
    "\n",
    "        # Test normal function route...\n",
    "        # result = await process(lambda *a, **p: print(*a, **p), *args, **params)\n",
    "\n",
    "        print('>>>', time.time() - start)\n",
    "        return result\n",
    "\n",
    "    return helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from datetime import datetime\n",
    "\n",
    "class URL(BaseModel):\n",
    "    \"\"\" Holds an url and its domain name.\n",
    "\n",
    "    If domain name is not specified, it will be guessed from the url\n",
    "\n",
    "    Fields:\n",
    "        url: str\n",
    "        domain: Optional[str]\n",
    "    \"\"\"\n",
    "    url: str\n",
    "    domain: Optional[str] = None\n",
    "\n",
    "    def __init__(self, **data: Any) -> None:\n",
    "        super().__init__(**data)\n",
    "        parsed_domain = domain_pattern.findall(self.url)\n",
    "\n",
    "        if self.domain is None and len(parsed_domain):\n",
    "            # auto fills domain name if not provided\n",
    "            self.domain = parsed_domain[0]\n",
    "\n",
    "\n",
    "class HTMLData(MongoModel):\n",
    "    \"\"\" Builds a html data representation\n",
    "\n",
    "    Fields:\n",
    "        url: URL\n",
    "        html: str\n",
    "        create_dt: datetime\n",
    "        job_id: Optional[str]\n",
    "        keywords: Optional[List[str]] = []\n",
    "    \"\"\"\n",
    "    url: URL\n",
    "    html: str\n",
    "    create_dt: datetime\n",
    "    job_id: Optional[str]\n",
    "    keywords: Optional[List[str]] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic(period):\n",
    "    def scheduler(fcn):\n",
    "\n",
    "        async def wrapper(*args, **kwargs):\n",
    "\n",
    "            while True:\n",
    "                asyncio.create_task(fcn(*args, **kwargs))\n",
    "                await asyncio.sleep(period)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from typing import Callable\n",
    "import aiohttp\n",
    "\n",
    "class AsyncIterator:\n",
    "    def __init__(self, seq):\n",
    "        self.iter = iter(seq)\n",
    "\n",
    "    def __aiter__(self):\n",
    "        return self\n",
    "\n",
    "    async def __anext__(self):\n",
    "        try:\n",
    "            return next(self.iter)\n",
    "        except StopIteration:\n",
    "            raise StopAsyncIteration\n",
    "\n",
    "\n",
    "class BaseSpiderService(ABC):\n",
    "    \"\"\" Defines common interface for spider services.\n",
    "    \"\"\"\n",
    "\n",
    "    def get(self, data_src: URL) -> Any:\n",
    "        return NotImplemented\n",
    "\n",
    "    def get_many(self, data_src: List[URL], rules: Any) -> Any:\n",
    "        return NotImplemented\n",
    "\n",
    "class HTMLSpiderService(BaseSpiderService):\n",
    "\n",
    "    def __init__(self, session: aiohttp.ClientSession, job_id: str = None):\n",
    "        BaseSpiderService.__init__(self)\n",
    "        self.session = session\n",
    "        self.html_data: List[HTMLData] = []\n",
    "        self.job_id = job_id\n",
    "        self.page_count = 0\n",
    "\n",
    "    async def get(self, data_src: URL) -> None:\n",
    "        async with self.session.get(data_src.url) as response:\n",
    "            html = await response.text()\n",
    "            return html\n",
    "\n",
    "    async def get_many(self, data_src: List[str], rules: ScrapeRules,\n",
    "                       async_db_action: Callable = None, async_in_progress_callback: Callable = None,\n",
    "                       async_job_done_callback: Callable = None, execute_job_callback_interval: int = 1,\n",
    "                       **kwargs) -> None:\n",
    "        \"\"\" Get html data given the data source\n",
    "        \n",
    "        Pass callback coroutines to this method when using a BackgroundTask scheduler.\n",
    "\n",
    "        Args: \n",
    "            data_src: List[str]\n",
    "            rules: ScrapeRules\n",
    "            async_in_progress_callback: corountine for handling job status during scraping\n",
    "            async_job_done_callback: corountine for handling job status after\n",
    "            async_db_action: coroutine for handling database operations\n",
    "            execute_job_callback_interval: time interval for executing in_progress_callback\n",
    "            kwargs: arguments for callbacks\n",
    "        \"\"\"\n",
    "        self.html_data = []\n",
    "        self.page_count = 0\n",
    "        \n",
    "        loop = asyncio.get_running_loop()\n",
    "        \n",
    "        async def scrape(url):\n",
    "            target_url = URL(url=url)\n",
    "            html = await self.get(target_url)\n",
    "            html_data = HTMLData(url=target_url, html=html,\n",
    "                                 create_dt=datetime.now(),\n",
    "                                 job_id=self.job_id)\n",
    "            self.page_count += 1\n",
    "            self.html_data.append(html_data)\n",
    "            \n",
    "        @periodic(1)\n",
    "        async def tick():\n",
    "#             if async_in_progress_callback:\n",
    "            await asyncio.sleep(1)\n",
    "            print(\"tik tok\")\n",
    "        \n",
    "        def unblock_periodic_task(task):\n",
    "            return lambda task: task.cancel()\n",
    "            \n",
    "        async def execute_db_action_after(tasks, periodic_task):\n",
    "            await asyncio.gather(*tasks)\n",
    "            if async_db_action:\n",
    "                await async_db_action(data=self.html_data, **kwargs)\n",
    "                periodic_task.cancel()\n",
    "            print(\"done\")\n",
    "        \n",
    "        tasks = [scrape(url) for url in data_src]\n",
    "        periodic_in_progress_task = loop.create_task(tick())\n",
    "#         loop.call_later(1, unblock_periodic_task(periodic_in_progress_task))\n",
    "        db_task = asyncio.create_task(execute_db_action_after(tasks, periodic_in_progress_task))\n",
    "        \n",
    "#         await periodic_in_progress_task\n",
    "        await db_task\n",
    "        \n",
    "            \n",
    "        return self.html_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = aiohttp.ClientSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider = HTMLSpiderService(session, job_id=str(uuid4()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_spec = JobSpecification(\n",
    "    urls=[\n",
    "          \"http://www.taobao.com\",\n",
    "          \"http://www.baidu.com\",\n",
    "          'http://www.guancha.cn',\n",
    "          'http://www.sina.com.cn'],\n",
    "    job_type=JobType.BASIC_PAGE_SCRAPING,\n",
    "    scrape_rules=ScrapeRules(\n",
    "        sizelimit=SizeLimit(max_pages=10)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = create_client(username=\"admin\", password=\"root\", host=\"localhost\", port=27017, db_name=\"spiderDB\")\n",
    "test_collection = client.spiderDB.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "data = asyncio.run(spider.get_many(job_spec.urls, job_spec.scrape_rules, async_db_action=HTMLData.insert_many, collection=test_collection))\n",
    "used_time = time.time() - start_time\n",
    "print(f\"used {used_time} seconds\")\n",
    "print(f\"Collected {len(data)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "async def test_scrape(spider):\n",
    "    data = await spider.get_many(job_spec.urls, job_spec.scrape_rules, async_db_action=HTMLData.insert_many, collection=test_collection)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def factorial(name, number):\n",
    "    f = 1\n",
    "    for i in range(2, number + 1):\n",
    "        print(f\"Task {name}: Compute factorial({number}), currently i={i}...\")\n",
    "        await asyncio.sleep(1)\n",
    "        f *= i\n",
    "    print(f\"Task {name}: factorial({number}) = {f}\")\n",
    "    return f\n",
    "\n",
    "async def main():\n",
    "    # Schedule three calls *concurrently*:\n",
    "    L = await asyncio.gather(\n",
    "        factorial(\"A\", 2),\n",
    "        factorial(\"B\", 3),\n",
    "        factorial(\"C\", 4),\n",
    "    )\n",
    "    print(L)\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def find_all(test_collection):\n",
    "    print(test_collection)\n",
    "    data = [HTMLData(**d) async for d in test_collection.find({})]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = HTMLData(url=URL(url='http://www.bbc.com'), html='<p>news</p>', create_dt=datetime(1976, 5, 28, 4, 21, 11, 901000), job_id='1', keywords=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.run(test_data.save(client.spiderDB, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.run(HTMLData.get(client.spiderDB.test, {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [HTMLData(url=URL(url=f'http://www.{s}.com'), html=f'<p>{s}</p>', create_dt=datetime.now(), job_id=str(uuid4()), keywords=[]) for s in \"abcde\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.run(HTMLData.insert_many(client.spiderDB.test, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestStatus(str, Enum):\n",
    "    \"\"\" Maps common HTTP status codes to their corresponding meanings\n",
    "    \"\"\"\n",
    "    CREATED = 'created'\n",
    "    WAITING = 'waiting'\n",
    "    SUCCESS = 'success'\n",
    "    TIMEOUT = 'timeout'\n",
    "    CLIENT_ERROR = 'client_error'\n",
    "    SERVER_ERROR = 'server_error'\n",
    "    BAD_REQUEST = 'bad_request'\n",
    "    UNAUTHORIZED = 'unauthorized'\n",
    "    FORBIDDEN = 'forbidden'\n",
    "    NOT_FOUND = 'not_found'\n",
    "    INTERNAL_SERVER_ERROR = 'internal_server_error'\n",
    "    TOO_MANY_REQUESTS = 'too_many_requests'\n",
    "    REDIRECTED = 'redirected'\n",
    "\n",
    "    @classmethod\n",
    "    def from_status_code(cls, status_code: int):\n",
    "        \"\"\" Convert a status code to its string representation \"\"\"\n",
    "        # checks whether status code is between 200 and 206, but\n",
    "        # range is exclusive on the right hand side\n",
    "        if 200 <= status_code <= 206:\n",
    "            return cls.SUCCESS\n",
    "        elif 300 <= status_code <= 309:\n",
    "            return cls.REDIRECTED\n",
    "        elif status_code == 400:\n",
    "            return cls.BAD_REQUEST\n",
    "        elif status_code == 401:\n",
    "            return cls.UNAUTHORIZED\n",
    "        elif status_code == 403:\n",
    "            return cls.FORBIDDEN\n",
    "        elif status_code == 404:\n",
    "            return cls.NOT_FOUND\n",
    "        elif status_code == 429:\n",
    "            return cls.TOO_MANY_REQUESTS\n",
    "        elif 405 <= status_code <= 452:\n",
    "            return cls.CLIENT_ERROR\n",
    "        elif status_code == 429:\n",
    "            return cls.TOO_MANY_REQUESTS\n",
    "        elif status_code == 500:\n",
    "            return cls.INTERNAL_SERVER_ERROR\n",
    "        elif 501 <= status_code <= 511:\n",
    "            return cls.SERVER_ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RequestStatus.from_status_code(403)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional, List, Any, Union\n",
    "\n",
    "class ParseRuleType(str, Enum):\n",
    "    \"\"\" Parse rule types supported by parsers \n",
    "    \n",
    "    One of:\n",
    "        XPATH,\n",
    "        CSS_SELECTOR,\n",
    "        REGEX\n",
    "    \"\"\"\n",
    "    XPATH: str = 'xpath'\n",
    "    CSS_SELECTOR: str = 'css_selector'\n",
    "    REGEX: str = 'regex'\n",
    "\n",
    "        \n",
    "class ParseRule(BaseModel):\n",
    "    \"\"\" Defines the parse rule for a parser\n",
    "    \n",
    "    Fields:\n",
    "        field_name: str\n",
    "        field_value: str\n",
    "        rule: str\n",
    "        rule_type: ParseRuleType        \n",
    "    \"\"\"\n",
    "    field_name: str\n",
    "    field_value: str\n",
    "    rule: str\n",
    "    rule_type: ParseRuleType\n",
    "\n",
    "\n",
    "class ParseResult(BaseModel):\n",
    "    \"\"\" Defines the parse result from a parser\n",
    "    \n",
    "    Fields:\n",
    "        field_name: str\n",
    "        field_value: str  \n",
    "    \"\"\"\n",
    "    name: str\n",
    "    value: str\n",
    "\n",
    "        \n",
    "class URL(BaseModel):\n",
    "    \"\"\" Holds an url and its domain name.\n",
    "\n",
    "    If domain name is not specified, it will be guessed from the url\n",
    "\n",
    "    Fields:\n",
    "        url: str\n",
    "        domain: Optional[str]\n",
    "    \"\"\"\n",
    "    url: str\n",
    "    domain: Optional[str] = None\n",
    "\n",
    "    def __init__(self, **data: Any) -> None:\n",
    "        super().__init__(**data)\n",
    "        parsed_domain = domain_pattern.findall(self.url)\n",
    "\n",
    "        if self.domain is None and len(parsed_domain):\n",
    "            # auto fills domain name if not provided\n",
    "            self.domain = parsed_domain[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from abc import ABC\n",
    "from typing import Any, List, Dict\n",
    "# from ..models.data_models import ParseRule, ParseResult, URL\n",
    "\n",
    "\n",
    "\n",
    "class BaseParser(ABC):\n",
    "\n",
    "    def parse(self, text: str, rules: List[ParseRule]) -> List[ParseResult]:\n",
    "        return NotImplemented\n",
    "\n",
    "\n",
    "class LinkParser(BaseParser):\n",
    "    _parser = BeautifulSoup\n",
    "\n",
    "    def parse(self, text: str) -> List[URL]:\n",
    "        return NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(LinkParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cuiqingcai.com/1319.html\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_page = requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = BeautifulSoup(test_page, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = page.select(\"*:is(p)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [content.text.strip() for content in contents if len(content.text.strip()) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml.html import fromstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_tree = lxml.etree.HTML(test_page)\n",
    "html_tree = fromstring(test_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = html_tree.xpath('//article/div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str.strip??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in contents:\n",
    "    print(c.text_content().strip('\\n '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = links[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(link, \"href\")\n",
    "link.text_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "css_select = getattr(page, 'select')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath_select = html_tree.xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "css_select(\"a[href^=http]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath_select('(//body//a)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in links:\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URL(BaseModel):\n",
    "    \"\"\" Holds an url and its domain name.\n",
    "\n",
    "    If domain name is not specified, it will be guessed from the url\n",
    "\n",
    "    Fields:\n",
    "        url: str\n",
    "        domain: Optional[str]\n",
    "    \"\"\"\n",
    "    url: str\n",
    "    domain: Optional[str] = None\n",
    "\n",
    "    def __init__(self, domain_pattern = re.compile(\"^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/?\\n]+)\"), **data: Any) -> None:\n",
    "        super().__init__(**data)\n",
    "        parsed_domain = domain_pattern.findall(self.url)\n",
    "\n",
    "        if self.domain is None and len(parsed_domain):\n",
    "            # auto fills domain name if not provided\n",
    "            self.domain = parsed_domain[0]\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.__repr__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "domain_pattern = re.compile(\"^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/?\\n]+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = URL(url=\"https://www.google.com.hk/\")\n",
    "url2 = URL(url=\"https://www.google.com.hk/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_set.add(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_set.add(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = URL(url=\"/s?wd=%E6%9D%8E%E5%8D%8E%E6%98%8E&rsv_idx=2&tn=baiduhome_pg&ie=utf-8&rsv_cq=beautifulsoup&rsv_dl=0_right_recommends_merge_20826&rsv_pq=faaa1a190003f124&oq=beautifulsoup&rsv_t=bbf8OkBM1xh84iXdDDdaz41CQON4kTR7nMiUX74PVrfLJY0TjYocHz6G%2BUASur4Iv%2B6Y&euri=ac2c31061cbe4a86b95a0086cc39f6e1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup.select(BeautifulSoup(test_page), 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(page, 'select')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(etree.HTML(test_page),'xpath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_tree = etree.HTML(test_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_tree.xpath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(etree.HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_tree.cssselect('a[href^=http]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = html_tree.cssselect('a[href^=http]')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = partial(BeautifulSoup.select, etree.HTML(test_page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "#     \"https://www.google.com/search?q=scrapy&sxsrf=ALeKk03bcpnii8K22lvJxH--rR2KqJXLbw:1623390101257&ei=lffCYLukD5Hl-gTVmL3ADQ&start=10&sa=N&ved=2ahUKEwj7wsSy747xAhWRsp4KHVVMD9gQ8tMDegQIARA7\", \n",
    "#     \"https://www.google.com/search?q=scrapy&sxsrf=ALeKk03bcpnii8K22lvJxH--rR2KqJXLbw:1623390101257&ei=lffCYLukD5Hl-gTVmL3ADQ&start=40&sa=N&ved=2ahUKEwj7wsSy747xAhWRsp4KHVVMD9gQ8tMDegQIARBB\",\n",
    "    \n",
    "    \"https://www.baidu.com/s?wd=scrapy&pn=20&oq=scrapy&tn=baiduhome_pg&ie=utf-8&usm=4&rsv_idx=2&rsv_pq=9e4bcc9400012ea1&rsv_t=2516n0hjetaZMGZKNUuRvN1VMSf30%2B5WZ%2FXekpewX2ta1xYQC1ywaqoTETrYz2WZFvgK&gpc=stf&tfflag=0&rsv_page=1\",\n",
    "    \"https://www.baidu.com/s?wd=scrapy&pn=10&oq=scrapy&tn=baiduhome_pg&ie=utf-8&rsv_idx=2&rsv_pq=c145f364000074cf&rsv_t=1060Jw36QPVOtvTjzEupgZ1u1SF1HXn%2BLIKV%2BqbBDYiKJE5kEB7m4%2BeL8wCauWXjWOR5&gpc=stf%3D1622785207%2C1623390007%7Cstftype%3D1&tfflag=1&rsv_page=1\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1, param2 = [set(u.split(\"?\")[-1].split(\"&\")) for u in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'gpc=stf',\n",
       "  'ie=utf-8',\n",
       "  'oq=scrapy',\n",
       "  'pn=20',\n",
       "  'rsv_idx=2',\n",
       "  'rsv_page=1',\n",
       "  'rsv_pq=9e4bcc9400012ea1',\n",
       "  'rsv_t=2516n0hjetaZMGZKNUuRvN1VMSf30%2B5WZ%2FXekpewX2ta1xYQC1ywaqoTETrYz2WZFvgK',\n",
       "  'tfflag=0',\n",
       "  'tn=baiduhome_pg',\n",
       "  'usm=4',\n",
       "  'wd=scrapy'},\n",
       " {'gpc=stf%3D1622785207%2C1623390007%7Cstftype%3D1',\n",
       "  'ie=utf-8',\n",
       "  'oq=scrapy',\n",
       "  'pn=10',\n",
       "  'rsv_idx=2',\n",
       "  'rsv_page=1',\n",
       "  'rsv_pq=c145f364000074cf',\n",
       "  'rsv_t=1060Jw36QPVOtvTjzEupgZ1u1SF1HXn%2BLIKV%2BqbBDYiKJE5kEB7m4%2BeL8wCauWXjWOR5',\n",
       "  'tfflag=1',\n",
       "  'tn=baiduhome_pg',\n",
       "  'wd=scrapy'})"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param1,param2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "paging_param_pattern = re.compile(\"^(start|page|p|pn|\\w+)=\\d{1,3}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-304-a2787f15740c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparam1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpaging_param_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpaging_param_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for s1, s2 in (param1, param2):\n",
    "    if paging_param_pattern.match(s1) and paging_param_pattern.match(s2):\n",
    "        print(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "cn_time_string_extractors = {\n",
    "            re.compile('\\d{1,2}秒前'):\n",
    "                lambda now, time_str: now -\n",
    "                timedelta(seconds=int(re.search('\\d+', time_str).group(0))),\n",
    "            re.compile('\\d{1,2}分钟前'):\n",
    "                lambda now, time_str: now -\n",
    "                timedelta(minutes=int(re.search('\\d+', time_str).group(0))),\n",
    "            re.compile('\\d{1,2}小时前'):\n",
    "                lambda now, time_str: now -\n",
    "                timedelta(hours=int(re.search('\\d+', time_str).group(0))),\n",
    "            re.compile('\\d{1,2}天前'):\n",
    "                lambda now, time_str: now -\n",
    "                timedelta(days=int(re.search('\\d+', time_str).group(0))),\n",
    "            re.compile('昨天\\d{1,2}:\\d{1,2}'):\n",
    "                lambda now, time_str: datetime(\n",
    "                    now.year, now.month, now.day-1,\n",
    "                    int(re.findall('\\d+', time_str)[0]),\n",
    "                    int(re.findall('\\d+', time_str)[1])\n",
    "                    \n",
    "            ),\n",
    "            re.compile('\\d{1,2}月\\d{1,2}日'):\n",
    "                lambda now, time_str: datetime(\n",
    "                    now.year,\n",
    "                    int(re.findall('\\d+', time_str)[0]),\n",
    "                        int(re.findall('\\d+', time_str)[1])),\n",
    "            \n",
    "            re.compile('\\d{1,2}年\\d{1,2}月\\d{1,2}日'):\n",
    "                lambda now, time_str: datetime(\n",
    "                    *(re.findall('\\d+', time_str))\n",
    "                )\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = [\"58分钟前\", '1小时前', '昨天13:15', '6月5日', '5天前']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-16 18:41:43.457528\n",
      "2021-06-16 18:39:43.457528\n",
      "2021-06-15 13:15:00\n",
      "2021-06-05 00:00:00\n",
      "2021-06-11 19:39:43.457528\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "for time_str in ts:\n",
    "    for pattern in cn_time_string_extractors:\n",
    "        if pattern.match(time_str):\n",
    "            converted = cn_time_string_extractors[pattern](now, time_str)\n",
    "            print(converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 1, 1, 0, 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime(2021,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.compile('\\d{1,2}秒前'):\\\n",
    "                lambda now, time_str: now -\n",
    "                timedelta(seconds=int(re.search('\\d+', time_str).group(0))),\n",
    "            re.compile('\\d{1,2}分钟前'):\\\n",
    "                lambda now, time_str: now -\n",
    "                timedelta(minutes=int(re.search('\\d+', time_str).group(0))),\n",
    "            re.compile('\\d{1,2}小时前'):\\\n",
    "                lambda now, time_str: now -\n",
    "                timedelta(hours=int(re.search('\\d+', time_str).group(0))),\n",
    "            re.compile('\\d{1,2}天前'):\\\n",
    "                lambda now, time_str: now -\n",
    "                timedelta(days=int(re.search('\\d+', time_str).group(0))),\n",
    "            re.compile('\\d{1,2}月\\d{1,2}日'):\\\n",
    "                lambda now, time_str: datetime.date(\n",
    "                    now.year,\n",
    "                    int(re.findall('\\d+', time_str)[0],\n",
    "                        int(re.findall('\\d+', time_str)[1]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = [\"\"\"\n",
    "据“中国载人航天”微信公众号消息，\"\"\",\n",
    "\"\"\"中国载人航天工程办公室：\"\"\",\n",
    "\"\"\"执行神舟十二号载人航天飞行任务的载人飞船及长征二号F遥十二运载火箭完成出厂前所有研制工作，\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_pattern = re.compile(\"(?!(执行))*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 0), match=''>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(exclude_pattern, paragraph[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(18, 30), match='中国载人航天工程办公室：'>\n",
      "<re.Match object; span=(77, 110), match='日前，已分批安全运抵酒泉卫星发射中心，开展发射场区总装和测试工作。'>\n",
      "<re.Match object; span=(111, 114), match='目前，'>\n",
      "<re.Match object; span=(115, 127), match='发射场设施设备状态良好，'>\n"
     ]
    }
   ],
   "source": [
    "s = '''据“中国载人航天”微信公众号消息，\n",
    "中国载人航天工程办公室：\n",
    "执行神舟十二号载人航天飞行任务的载人飞船及长征二号F遥十二运载火箭完成出厂前所有研制工作，\n",
    "日前，已分批安全运抵酒泉卫星发射中心，开展发射场区总装和测试工作。\n",
    "目前，\n",
    "发射场设施设备状态良好，\n",
    "参试各系统正在有序开展各项任务准备，\n",
    "执行本次载人航天飞行任务的航天员乘组正在进行强化训练。'''\n",
    "for match in re.finditer('^((?!微信公众号|任务).)*$', s, flags=re.M):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('^((?!计算机).)*$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(pattern, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return an iterator over all non-overlapping matches in the\n",
       "string.  For each match, the iterator returns a Match object.\n",
       "\n",
       "Empty matches are included in the result.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.7/re.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "re.finditer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
