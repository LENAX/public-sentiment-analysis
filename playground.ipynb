{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "loop = asyncio.get_event_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from motor.motor_asyncio import AsyncIOMotorClient\n",
    "\n",
    "\n",
    "def create_client(host: str, username: str,\n",
    "                  password: str, port: int,\n",
    "                  db_name: str) -> AsyncIOMotorClient:\n",
    "    return AsyncIOMotorClient(\n",
    "            f\"mongodb://{username}:{password}@{host}:{port}/{db_name}?authSource=admin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "\n",
    "class AsyncCRUDBase(object):\n",
    "\n",
    "    @staticmethod\n",
    "    async def get(db: Any, query: Any, **kwargs):\n",
    "        return NotImplemented\n",
    "\n",
    "    @staticmethod\n",
    "    async def delete(db: Any, query: Any, **kwargs):\n",
    "        return NotImplemented\n",
    "\n",
    "    @staticmethod\n",
    "    async def insert_many(db: Any, data: Any, **kwargs):\n",
    "        return NotImplemented\n",
    "\n",
    "    async def save(self, db, collection):\n",
    "        return NotImplemented\n",
    "\n",
    "\n",
    "class AsyncMongoCRUDBase(AsyncCRUDBase):\n",
    "    \"\"\" Provides minimal support for writing to MongoDB\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    async def get(collection: Any,  query: Any, **kwargs) -> List[object]:\n",
    "        result = [data async for data in collection.find(query)]\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    async def delete(collection: Any, query: Any, **kwargs):\n",
    "        return NotImplemented\n",
    "\n",
    "    @staticmethod\n",
    "    async def insert_many(collection: Any, data: Any, **kwargs):\n",
    "        await collection.insert_many(data)\n",
    "\n",
    "    async def save(self, collection):\n",
    "        return NotImplemented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from bson import ObjectId\n",
    "from datetime import datetime, date\n",
    "from typing import Optional, List\n",
    "\n",
    "class MongoModel(BaseModel, AsyncMongoCRUDBase):\n",
    "\n",
    "    class Config:\n",
    "        allow_population_by_field_name = True\n",
    "        json_encoders = {\n",
    "            datetime: lambda dt: dt.isoformat(),\n",
    "            ObjectId: lambda oid: str(oid),\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_mongo(cls, data: dict):\n",
    "        \"\"\"We must convert _id into \"id\". \"\"\"\n",
    "        if not data:\n",
    "            return data\n",
    "        id = data.pop('_id', None)\n",
    "        return cls(**dict(data, id=id))\n",
    "\n",
    "    def mongo(self, **kwargs):\n",
    "        exclude_unset = kwargs.pop('exclude_unset', True)\n",
    "        by_alias = kwargs.pop('by_alias', True)\n",
    "\n",
    "        parsed = self.dict(\n",
    "            exclude_unset=exclude_unset,\n",
    "            by_alias=by_alias,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Mongo uses `_id` as default key. We should stick to that as well.\n",
    "        if '_id' not in parsed and 'id' in parsed:\n",
    "            parsed['_id'] = parsed.pop('id')\n",
    "\n",
    "        return parsed\n",
    "    \n",
    "    @staticmethod\n",
    "    async def insert_many(collection: Any, data: List[AsyncMongoCRUDBase], **kwargs):\n",
    "        await collection.insert_many([d.mongo() for d in data])\n",
    "\n",
    "    @classmethod\n",
    "    async def get(cls, collection: Any,  query: Any, **kwargs) -> List[object]:\n",
    "        result = [cls.from_mongo(data) async for data in collection.find(query)]\n",
    "        return result\n",
    "\n",
    "    async def save(self, db, collection_name:str):\n",
    "        try:\n",
    "            await db[collection_name].insert_one(self.mongo())\n",
    "        except Exception as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "domain_pattern = re.compile(\"^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/?\\n]+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class JobState(str, Enum):\n",
    "    PENDING= 'pending'\n",
    "    DONE = 'done'\n",
    "    WORKING = 'working'\n",
    "    FAILED = 'failed'\n",
    "\n",
    "\n",
    "class ContentType(str, Enum):\n",
    "    WEBPAGE: str = 'webpage'\n",
    "    IMAGE: str = 'image'\n",
    "    AUDIO: str = 'audio'\n",
    "    VIDEO: str = 'video'\n",
    "\n",
    "\n",
    "class JobType(str, Enum):\n",
    "    \"\"\" Job types supported by spiders\n",
    "\n",
    "    BASIC_PAGE_SCRAPING: only scrape the provided urls and return the html of those urls,\n",
    "    SEARCH_RESULT_AGGREGATION: perform searches on search engines or general search page and retrieve their results,\n",
    "    WEB_CRAWLING: Start from seed urls, follow all links available.\n",
    "    \"\"\"\n",
    "    BASIC_PAGE_SCRAPING: str = 'basic_page_scraping'\n",
    "    SEARCH_RESULT_AGGREGATION: str = 'search_result_aggregation'\n",
    "    # WEB_CRAWLING: str = 'web_crawling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordRules(BaseModel):\n",
    "    include: List[str] = []\n",
    "    exclude: List[str] = []\n",
    "\n",
    "\n",
    "class SizeLimit(BaseModel):\n",
    "    max_pages: Optional[int]\n",
    "    max_size: Optional[int]\n",
    "\n",
    "\n",
    "class TimeRange(BaseModel):\n",
    "    past_days: Optional[int]\n",
    "    date_before: Optional[date]\n",
    "    date_after: Optional[date]\n",
    "\n",
    "\n",
    "class RegexPattern(BaseModel):\n",
    "    patterns: Optional[List[str]] = []\n",
    "\n",
    "\n",
    "class ScrapeRules(BaseModel):\n",
    "    \"\"\" Describes rules a spider should follow\n",
    "\n",
    "    Fields:\n",
    "        keywords: Optional[KeywordRules]\n",
    "        size_limit: Optional[SizeLimit]\n",
    "        time_range: Optional[TimeRange]\n",
    "        regular_expressions: Optional[RegexPattern]\n",
    "        max_retry: Optional[int] = 1  \n",
    "    \"\"\"\n",
    "    keywords: Optional[KeywordRules]\n",
    "    size_limit: Optional[SizeLimit]\n",
    "    time_range: Optional[TimeRange]\n",
    "    regular_expressions: Optional[RegexPattern]\n",
    "    max_retry: Optional[int] = 1\n",
    "\n",
    "\n",
    "\n",
    "class JobSpecification(BaseModel):\n",
    "    \"\"\" Describes what kind of task a spider should perform\n",
    "\n",
    "    Fields:\n",
    "        urls: List[str]\n",
    "        job_type: JobType\n",
    "        scrape_rules: ScrapeRules\n",
    "        data_collection: str = 'test'\n",
    "    \"\"\"\n",
    "    urls: List[str]\n",
    "    job_type: JobType\n",
    "    scrape_rules: ScrapeRules\n",
    "    data_collection: str = 'test'\n",
    "    job_collection: str = \"jobs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Any\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class JobStatus(MongoModel):\n",
    "    job_id: str\n",
    "    create_dt: datetime\n",
    "    page_count: int = 0\n",
    "    time_consumed: Optional[timedelta]\n",
    "    current_state: JobState\n",
    "    specification: JobSpecification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JobSpecification(urls=['http://www.qq.com', 'http://www.taobao.com', 'http://www.baidu.com', 'http://www.guancha.cn', 'http://www.sina.com.cn', 'http://www.qq.com', 'http://www.taobao.com', 'http://www.baidu.com', 'http://www.guancha.cn', 'http://www.sina.com.cn', 'http://www.qq.com', 'http://www.taobao.com', 'http://www.baidu.com', 'http://www.guancha.cn', 'http://www.sina.com.cn', 'http://www.qq.com', 'http://www.taobao.com', 'http://www.baidu.com', 'http://www.guancha.cn', 'http://www.sina.com.cn', 'http://www.qq.com', 'http://www.taobao.com', 'http://www.baidu.com', 'http://www.guancha.cn', 'http://www.sina.com.cn'], job_type=<JobType.BASIC_PAGE_SCRAPING: 'basic_page_scraping'>, scrape_rules=ScrapeRules(keywords=None, size_limit=None, time_range=None, regular_expressions=None, max_retry=1), data_collection='test', job_collection='jobs')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_spec = JobSpecification(\n",
    "    urls=['http://www.qq.com',\n",
    "          \"http://www.taobao.com\",\n",
    "          \"http://www.baidu.com\",\n",
    "          'http://www.guancha.cn',\n",
    "          'http://www.sina.com.cn']*5,\n",
    "    job_type=JobType.BASIC_PAGE_SCRAPING,\n",
    "    scrape_rules=ScrapeRules(\n",
    "        sizelimit=SizeLimit(max_pages=10)\n",
    "    )\n",
    ")\n",
    "job_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JobStatus(job_id='5f8bd784-a939-4f4e-89c4-9985642d2271', create_dt=datetime.datetime(2021, 5, 31, 0, 47, 27, 189535), page_count=0, time_consumed=datetime.timedelta(0), current_state=<JobState.PENDING: 'pending'>, specification=JobSpecification(urls=['http://www.qq.com', 'http://www.taobao.com', 'http://www.baidu.com', 'http://www.guancha.cn', 'http://www.sina.com.cn', 'http://www.qq.com', 'http://www.taobao.com', 'http://www.baidu.com', 'http://www.guancha.cn', 'http://www.sina.com.cn', 'http://www.qq.com', 'http://www.taobao.com', 'http://www.baidu.com', 'http://www.guancha.cn', 'http://www.sina.com.cn', 'http://www.qq.com', 'http://www.taobao.com', 'http://www.baidu.com', 'http://www.guancha.cn', 'http://www.sina.com.cn', 'http://www.qq.com', 'http://www.taobao.com', 'http://www.baidu.com', 'http://www.guancha.cn', 'http://www.sina.com.cn'], job_type=<JobType.BASIC_PAGE_SCRAPING: 'basic_page_scraping'>, scrape_rules=ScrapeRules(keywords=None, size_limit=None, time_range=None, regular_expressions=None, max_retry=1), data_collection='test', job_collection='jobs'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "job_status = JobStatus(\n",
    "            job_id=str(uuid4()),\n",
    "            create_dt=datetime.now(),\n",
    "            page_count=0,\n",
    "            specification=job_spec,\n",
    "            current_state=JobState.PENDING,\n",
    "            time_consumed=timedelta(seconds=0))\n",
    "job_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "\n",
    "def timeit(func):\n",
    "    async def process(func, *args, **params):\n",
    "        if asyncio.iscoroutinefunction(func):\n",
    "            print('this function is a coroutine: {}'.format(func.__name__))\n",
    "            return await func(*args, **params)\n",
    "        else:\n",
    "            print('this is not a coroutine')\n",
    "            return func(*args, **params)\n",
    "\n",
    "    async def helper(*args, **params):\n",
    "        print('{}.time'.format(func.__name__))\n",
    "        start = time.time()\n",
    "        result = await process(func, *args, **params)\n",
    "\n",
    "        # Test normal function route...\n",
    "        # result = await process(lambda *a, **p: print(*a, **p), *args, **params)\n",
    "\n",
    "        print('>>>', time.time() - start)\n",
    "        return result\n",
    "\n",
    "    return helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from datetime import datetime\n",
    "\n",
    "class URL(BaseModel):\n",
    "    \"\"\" Holds an url and its domain name.\n",
    "\n",
    "    If domain name is not specified, it will be guessed from the url\n",
    "\n",
    "    Fields:\n",
    "        url: str\n",
    "        domain: Optional[str]\n",
    "    \"\"\"\n",
    "    url: str\n",
    "    domain: Optional[str] = None\n",
    "\n",
    "    def __init__(self, **data: Any) -> None:\n",
    "        super().__init__(**data)\n",
    "        parsed_domain = domain_pattern.findall(self.url)\n",
    "\n",
    "        if self.domain is None and len(parsed_domain):\n",
    "            # auto fills domain name if not provided\n",
    "            self.domain = parsed_domain[0]\n",
    "\n",
    "\n",
    "class HTMLData(MongoModel):\n",
    "    \"\"\" Builds a html data representation\n",
    "\n",
    "    Fields:\n",
    "        url: URL\n",
    "        html: str\n",
    "        create_dt: datetime\n",
    "        job_id: Optional[str]\n",
    "        keywords: Optional[List[str]] = []\n",
    "    \"\"\"\n",
    "    url: URL\n",
    "    html: str\n",
    "    create_dt: datetime\n",
    "    job_id: Optional[str]\n",
    "    keywords: Optional[List[str]] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic(period):\n",
    "    def scheduler(fcn):\n",
    "\n",
    "        async def wrapper(*args, **kwargs):\n",
    "\n",
    "            while True:\n",
    "                asyncio.create_task(fcn(*args, **kwargs))\n",
    "                await asyncio.sleep(period)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from typing import Callable\n",
    "import aiohttp\n",
    "\n",
    "class AsyncIterator:\n",
    "    def __init__(self, seq):\n",
    "        self.iter = iter(seq)\n",
    "\n",
    "    def __aiter__(self):\n",
    "        return self\n",
    "\n",
    "    async def __anext__(self):\n",
    "        try:\n",
    "            return next(self.iter)\n",
    "        except StopIteration:\n",
    "            raise StopAsyncIteration\n",
    "\n",
    "\n",
    "class BaseSpiderService(ABC):\n",
    "    \"\"\" Defines common interface for spider services.\n",
    "    \"\"\"\n",
    "\n",
    "    def get(self, data_src: URL) -> Any:\n",
    "        return NotImplemented\n",
    "\n",
    "    def get_many(self, data_src: List[URL], rules: Any) -> Any:\n",
    "        return NotImplemented\n",
    "\n",
    "class HTMLSpiderService(BaseSpiderService):\n",
    "\n",
    "    def __init__(self, session: aiohttp.ClientSession, job_id: str = None):\n",
    "        BaseSpiderService.__init__(self)\n",
    "        self.session = session\n",
    "        self.html_data: List[HTMLData] = []\n",
    "        self.job_id = job_id\n",
    "        self.page_count = 0\n",
    "\n",
    "    async def get(self, data_src: URL) -> None:\n",
    "        async with self.session.get(data_src.url) as response:\n",
    "            html = await response.text()\n",
    "            return html\n",
    "\n",
    "    async def get_many(self, data_src: List[str], rules: ScrapeRules,\n",
    "                       async_db_action: Callable = None, async_in_progress_callback: Callable = None,\n",
    "                       async_job_done_callback: Callable = None, execute_job_callback_interval: int = 1,\n",
    "                       **kwargs) -> None:\n",
    "        \"\"\" Get html data given the data source\n",
    "        \n",
    "        Pass callback coroutines to this method when using a BackgroundTask scheduler.\n",
    "\n",
    "        Args: \n",
    "            data_src: List[str]\n",
    "            rules: ScrapeRules\n",
    "            async_in_progress_callback: corountine for handling job status during scraping\n",
    "            async_job_done_callback: corountine for handling job status after\n",
    "            async_db_action: coroutine for handling database operations\n",
    "            execute_job_callback_interval: time interval for executing in_progress_callback\n",
    "            kwargs: arguments for callbacks\n",
    "        \"\"\"\n",
    "        self.html_data = []\n",
    "        self.page_count = 0\n",
    "        \n",
    "        loop = asyncio.get_running_loop()\n",
    "        \n",
    "        async def scrape(url):\n",
    "            target_url = URL(url=url)\n",
    "            html = await self.get(target_url)\n",
    "            html_data = HTMLData(url=target_url, html=html,\n",
    "                                 create_dt=datetime.now(),\n",
    "                                 job_id=self.job_id)\n",
    "            self.page_count += 1\n",
    "            self.html_data.append(html_data)\n",
    "            \n",
    "        @periodic(1)\n",
    "        async def tick():\n",
    "#             if async_in_progress_callback:\n",
    "            await asyncio.sleep(1)\n",
    "            print(\"tik tok\")\n",
    "        \n",
    "        def unblock_periodic_task(task):\n",
    "            return lambda task: task.cancel()\n",
    "            \n",
    "        async def execute_db_action_after(tasks, periodic_task):\n",
    "            await asyncio.gather(*tasks)\n",
    "            if async_db_action:\n",
    "                await async_db_action(data=self.html_data, **kwargs)\n",
    "                periodic_task.cancel()\n",
    "            print(\"done\")\n",
    "        \n",
    "        tasks = [scrape(url) for url in data_src]\n",
    "        periodic_in_progress_task = loop.create_task(tick())\n",
    "#         loop.call_later(1, unblock_periodic_task(periodic_in_progress_task))\n",
    "        db_task = asyncio.create_task(execute_db_action_after(tasks, periodic_in_progress_task))\n",
    "        \n",
    "#         await periodic_in_progress_task\n",
    "        await db_task\n",
    "        \n",
    "            \n",
    "        return self.html_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = aiohttp.ClientSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider = HTMLSpiderService(session, job_id=str(uuid4()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_spec = JobSpecification(\n",
    "    urls=[\n",
    "          \"http://www.taobao.com\",\n",
    "          \"http://www.baidu.com\",\n",
    "          'http://www.guancha.cn',\n",
    "          'http://www.sina.com.cn'],\n",
    "    job_type=JobType.BASIC_PAGE_SCRAPING,\n",
    "    scrape_rules=ScrapeRules(\n",
    "        sizelimit=SizeLimit(max_pages=10)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tik tok\n"
     ]
    }
   ],
   "source": [
    "client = create_client(username=\"admin\", password=\"root\", host=\"localhost\", port=27017, db_name=\"spiderDB\")\n",
    "test_collection = client.spiderDB.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "tik tok\n",
      "tik tok\n",
      "done\n",
      "used 2.119396209716797 seconds\n",
      "Collected 4 entries\n",
      "tik tok\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "data = asyncio.run(spider.get_many(job_spec.urls, job_spec.scrape_rules, async_db_action=HTMLData.insert_many, collection=test_collection))\n",
    "used_time = time.time() - start_time\n",
    "print(f\"used {used_time} seconds\")\n",
    "print(f\"Collected {len(data)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "async def test_scrape(spider):\n",
    "    data = await spider.get_many(job_spec.urls, job_spec.scrape_rules, async_db_action=HTMLData.insert_many, collection=test_collection)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1622383686.52325"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task A: Compute factorial(2), currently i=2...\n",
      "Task B: Compute factorial(3), currently i=2...\n",
      "Task C: Compute factorial(4), currently i=2...\n",
      "Task A: factorial(2) = 2\n",
      "Task B: Compute factorial(3), currently i=3...\n",
      "Task C: Compute factorial(4), currently i=3...\n",
      "Task B: factorial(3) = 6\n",
      "Task C: Compute factorial(4), currently i=4...\n",
      "Task C: factorial(4) = 24\n",
      "[2, 6, 24]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def factorial(name, number):\n",
    "    f = 1\n",
    "    for i in range(2, number + 1):\n",
    "        print(f\"Task {name}: Compute factorial({number}), currently i={i}...\")\n",
    "        await asyncio.sleep(1)\n",
    "        f *= i\n",
    "    print(f\"Task {name}: factorial({number}) = {f}\")\n",
    "    return f\n",
    "\n",
    "async def main():\n",
    "    # Schedule three calls *concurrently*:\n",
    "    L = await asyncio.gather(\n",
    "        factorial(\"A\", 2),\n",
    "        factorial(\"B\", 3),\n",
    "        factorial(\"C\", 4),\n",
    "    )\n",
    "    print(L)\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def find_all(test_collection):\n",
    "    print(test_collection)\n",
    "    data = [HTMLData(**d) async for d in test_collection.find({})]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = HTMLData(url=URL(url='http://www.bbc.com'), html='<p>news</p>', create_dt=datetime(1976, 5, 28, 4, 21, 11, 901000), job_id='1', keywords=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.run(test_data.save(client.spiderDB, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.run(HTMLData.get(client.spiderDB.test, {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [HTMLData(url=URL(url=f'http://www.{s}.com'), html=f'<p>{s}</p>', create_dt=datetime.now(), job_id=str(uuid4()), keywords=[]) for s in \"abcde\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.run(HTMLData.insert_many(client.spiderDB.test, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
